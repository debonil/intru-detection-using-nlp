{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rm09UPcjR9-g"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier, StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "\n",
        "# Read the training CSV file containing normal traffic data\n",
        "df_normal_train = pd.read_csv('train_normal.csv')\n",
        "\n",
        "# Read the testing CSV file containing abnormal traffic data\n",
        "df_test_abnormal = pd.read_csv('test_abnormal.csv')\n",
        "\n",
        "# Read the testing CSV file containing normal traffic data\n",
        "df_test_normal = pd.read_csv('test_normal.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "\n",
        "df_concat = pd.concat([df_normal_train, df_test_abnormal, df_test_normal])\n",
        "\n",
        "# Shuffle the concatenated dataframe\n",
        "df_shuffled = df_concat.sample(frac=1, random_state=42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jvebdEKSBj6",
        "outputId": "d1cd0ab1-7bb6-4109-c1cf-7e14fb8d6b6c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "preprocessed = []\n",
        "for request in df_shuffled['request']:\n",
        "    request = request.lower()\n",
        "    request = ''.join(e for e in request if e.isalnum() or e.isspace())\n",
        "    words = nltk.word_tokenize(request)\n",
        "    words = [w for w in words if w not in stop_words]\n",
        "    preprocessed.append(' '.join(words))\n",
        "\n",
        "for request in df_test_abnormal['request']:\n",
        "    request = request.lower()\n",
        "    request = ''.join(e for e in request if e.isalnum() or e.isspace())\n",
        "    words = nltk.word_tokenize(request)\n",
        "    words = [w for w in words if w not in stop_words]\n",
        "    preprocessed.append(' '.join(words))\n",
        "\n",
        "for request in df_test_normal['request']:\n",
        "    request = request.lower()\n",
        "    request = ''.join(e for e in request if e.isalnum() or e.isspace())\n",
        "    words = nltk.word_tokenize(request)\n",
        "    words = [w for w in words if w not in stop_words]\n",
        "    preprocessed.append(' '.join(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lra1x9RSEBt",
        "outputId": "f0a5cd96-0386-46fe-b440-15f11034fd0e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(preprocessed)\n",
        "dictionary = vectorizer.vocabulary_\n",
        "\n",
        "# Convert HTTP requests to feature vectors\n",
        "x_train_normal = vectorizer.transform(df_shuffled['request']).toarray()\n",
        "y_train_normal = df_shuffled['label']\n",
        "\n",
        "x_test_abnormal = vectorizer.transform(df_test_abnormal['request']).toarray()\n",
        "y_test_abnormal = df_test_abnormal['label'].dropna()\n",
        "\n",
        "x_test_normal = vectorizer.transform(df_test_normal['request']).toarray()\n",
        "y_test_normal = df_test_normal['label'].dropna()"
      ],
      "metadata": {
        "id": "sqaVinDzSGyn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Split the shuffled dataset into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_train_normal, y_train_normal, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the models to be trained and tested\n",
        "models = {\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"Support Vector Machine\": SVC(),\n",
        "    \"Multilayer Perceptron\": MLPClassifier()\n",
        "}\n",
        "\n",
        "ensemble_models = {\n",
        "    \"Bagging\": BaggingClassifier(),\n",
        "    \"AdaBoost\": AdaBoostClassifier(),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
        "    \"Stacking\": StackingClassifier(estimators=list(models.items()))\n",
        "}\n"
      ],
      "metadata": {
        "id": "Y0OfRh3OSI0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and test the models\n",
        "for name, model in models.items():\n",
        "    model.fit(x_train, y_train)\n",
        "    y_pred_abnormal = model.predict(x_test_abnormal)\n",
        "    accuracy = accuracy_score(y_test_abnormal, y_pred_abnormal)\n",
        "    print(f\"{name} accuracy on abnormal traffic data: {accuracy}\")\n",
        "    \n",
        "    y_pred_normal = model.predict(x_test_normal)\n",
        "    accuracy = accuracy_score(y_test_normal, y_pred_normal)\n",
        "    print(f\"{name} accuracy on normal traffic data: {accuracy}\")\n",
        "    \n",
        "    report = classification_report(y_test_abnormal, y_pred_abnormal)\n",
        "    print(f\"{name} classification report on abnormal traffic data:\\n{report}\")\n",
        "    \n",
        "    report = classification_report(y_test_normal, y_pred_normal)\n",
        "    print(f\"{name} classification report on normal traffic data:\\n{report}\")\n"
      ],
      "metadata": {
        "id": "0Z6exM4dSMIc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}